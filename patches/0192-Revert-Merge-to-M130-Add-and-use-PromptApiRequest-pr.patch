From 0000000000000000000000000000000000000000 Mon Sep 17 00:00:00 2001
From: fgei <fgei@gmail.com>
Date: Fri, 27 Sep 2024 02:49:48 +0000
Subject: [PATCH] Revert "Merge to M130: Add and use PromptApiRequest proto."

This reverts commit 797a2e0fd1b9b9cb3560ca2fac71ec2711bfe94d.
---
 .../ai/ai_manager_keyed_service_unittest.cc   |   2 -
 chrome/browser/ai/ai_test_utils.cc            |   6 -
 chrome/browser/ai/ai_test_utils.h             |   1 -
 chrome/browser/ai/ai_text_session.cc          | 239 +++++++---------
 chrome/browser/ai/ai_text_session.h           |  53 ++--
 chrome/browser/ai/ai_text_session_unittest.cc | 257 +++++-------------
 components/optimization_guide/core/BUILD.gn   |   4 -
 .../mock_optimization_guide_model_executor.cc |   8 -
 .../mock_optimization_guide_model_executor.h  |  13 -
 .../on_device_model_feature_adapter.cc        |   4 -
 .../on_device_model_feature_adapter.h         |   2 -
 .../core/model_execution/session_impl.cc      |  17 --
 .../core/model_execution/session_impl.h       |   4 -
 .../model_execution/substitution_unittest.cc  | 148 ----------
 .../core/optimization_guide_model_executor.h  |  11 -
 components/optimization_guide/proto/BUILD.gn  |  11 -
 .../proto/features/prompt_api.proto           |  45 ---
 .../on_device_model_execution_config.proto    |   4 -
 18 files changed, 188 insertions(+), 641 deletions(-)
 delete mode 100644 components/optimization_guide/proto/features/prompt_api.proto

diff --git a/chrome/browser/ai/ai_manager_keyed_service_unittest.cc b/chrome/browser/ai/ai_manager_keyed_service_unittest.cc
index cd2990a8e4d11..7c7a060a5f9d1 100644
--- a/chrome/browser/ai/ai_manager_keyed_service_unittest.cc
+++ b/chrome/browser/ai/ai_manager_keyed_service_unittest.cc
@@ -41,8 +41,6 @@ class AIManagerKeyedServiceTest : public AITestUtils::AITestBase {
             [&] { return std::make_unique<MockSessionWrapper>(&session_); });
     ON_CALL(session_, GetTokenLimits())
         .WillByDefault(AITestUtils::GetFakeTokenLimits);
-    ON_CALL(session_, GetOnDeviceFeatureMetadata())
-        .WillByDefault(AITestUtils::GetFakeFeatureMetadata);
   }
 
  private:
diff --git a/chrome/browser/ai/ai_test_utils.cc b/chrome/browser/ai/ai_test_utils.cc
index 115f21c63f8e5..004914a5f3460 100644
--- a/chrome/browser/ai/ai_test_utils.cc
+++ b/chrome/browser/ai/ai_test_utils.cc
@@ -90,9 +90,3 @@ const optimization_guide::TokenLimits& AITestUtils::GetFakeTokenLimits() {
   };
   return limits;
 }
-
-// static
-const optimization_guide::proto::Any& AITestUtils::GetFakeFeatureMetadata() {
-  static base::NoDestructor<optimization_guide::proto::Any> data;
-  return *data;
-}
diff --git a/chrome/browser/ai/ai_test_utils.h b/chrome/browser/ai/ai_test_utils.h
index b7d56dec5cb1f..567a18dca83f2 100644
--- a/chrome/browser/ai/ai_test_utils.h
+++ b/chrome/browser/ai/ai_test_utils.h
@@ -68,7 +68,6 @@ class AITestUtils {
 
   static std::string GetTypeURLForProto(std::string type_name);
   static const optimization_guide::TokenLimits& GetFakeTokenLimits();
-  static const optimization_guide::proto::Any& GetFakeFeatureMetadata();
 };
 
 #endif  // CHROME_BROWSER_AI_AI_TEST_UTILS_H_
diff --git a/chrome/browser/ai/ai_text_session.cc b/chrome/browser/ai/ai_text_session.cc
index caa10395a8da6..4e88774c35699 100644
--- a/chrome/browser/ai/ai_text_session.cc
+++ b/chrome/browser/ai/ai_text_session.cc
@@ -6,7 +6,6 @@
 
 #include <memory>
 #include <optional>
-#include <sstream>
 
 #include "base/check_op.h"
 #include "base/functional/bind.h"
@@ -22,7 +21,6 @@
 #include "components/optimization_guide/core/optimization_guide_model_executor.h"
 #include "components/optimization_guide/core/optimization_guide_util.h"
 #include "components/optimization_guide/proto/common_types.pb.h"
-#include "components/optimization_guide/proto/features/prompt_api.pb.h"
 #include "components/optimization_guide/proto/string_value.pb.h"
 #include "third_party/blink/public/mojom/ai/ai_manager.mojom-shared.h"
 #include "third_party/blink/public/mojom/ai/ai_text_session_info.mojom.h"
@@ -30,91 +28,47 @@
 
 namespace {
 
-using optimization_guide::proto::PromptApiMetadata;
-using optimization_guide::proto::PromptApiPrompt;
-using optimization_guide::proto::PromptApiRequest;
-using optimization_guide::proto::PromptApiRole;
-
-PromptApiRole ConvertRole(blink::mojom::AIAssistantInitialPromptRole role) {
-  switch (role) {
-    case blink::mojom::AIAssistantInitialPromptRole::kSystem:
-      return PromptApiRole::PROMPT_API_ROLE_SYSTEM;
-    case blink::mojom::AIAssistantInitialPromptRole::kUser:
-      return PromptApiRole::PROMPT_API_ROLE_USER;
-    case blink::mojom::AIAssistantInitialPromptRole::kAssistant:
-      return PromptApiRole::PROMPT_API_ROLE_ASSISTANT;
-  }
-}
-
-PromptApiPrompt MakePrompt(PromptApiRole role, const std::string& content) {
-  PromptApiPrompt prompt;
-  prompt.set_role(role);
-  prompt.set_content(content);
-  return prompt;
-}
-
-const char* FormatPromptRole(PromptApiRole role) {
-  switch (role) {
-    case PromptApiRole::PROMPT_API_ROLE_SYSTEM:
-      return "";  // No prefix for system prompt.
-    case PromptApiRole::PROMPT_API_ROLE_USER:
-      return "User: ";
-    case PromptApiRole::PROMPT_API_ROLE_ASSISTANT:
-      return "Model: ";
-    default:
-      NOTREACHED();
-  }
-}
-
-PromptApiMetadata ParseMetadata(const optimization_guide::proto::Any& any) {
-  PromptApiMetadata metadata;
-  if (any.type_url() == "type.googleapis.com/" + metadata.GetTypeName()) {
-    metadata.ParseFromString(any.value());
-  }
-  return metadata;
-}
-
-std::unique_ptr<optimization_guide::proto::StringValue> ToStringValue(
-    const PromptApiRequest& request) {
-  std::ostringstream oss;
-  auto FormatPrompts =
-      [](std::ostringstream& oss,
-         const google::protobuf::RepeatedPtrField<PromptApiPrompt> prompts) {
-        for (const auto& prompt : prompts) {
-          oss << FormatPromptRole(prompt.role()) << prompt.content() << "\n";
-        }
-      };
-  FormatPrompts(oss, request.initial_prompts());
-  FormatPrompts(oss, request.prompt_history());
-  FormatPrompts(oss, request.current_prompts());
-  if (request.current_prompts_size() > 0) {
-    oss << FormatPromptRole(PromptApiRole::PROMPT_API_ROLE_ASSISTANT);
-  }
-  auto value = std::make_unique<optimization_guide::proto::StringValue>();
-  value->set_value(oss.str());
-  return value;
-}
+// The format for the prompt and the context. The prompt structure helps the
+// model distinguish the roles in the previous conversation.
+
+// The template used to format each prompt input. Example:
+// ```
+// User: what is the opposite of large?
+// Model:
+// ```
+const char kPromptFormat[] = "%s: %s\n%s: ";
+// The template used to format each context entry. Example:
+// ```
+// User: what is the opposite of large?
+// Model: small.
+// ```
+const char kContextFormat[] = "%s%s\n";
+
+// The templates used to format different parts of the system prompt. Example:
+// ```
+// You are a professor of antonyms.      [system prompt]
+// User: what is the opposite of big?    [initial prompt for `User` role]
+// Model: small.                         [initial prompt for `Assistant` role]
+// User: what is the opposite of cheap?  [initial prompt for `User` role]
+// Model: expensive.                     [initial prompt for `Assistant` role]
+// ```
+const char kSystemPromptFormat[] = "%s\n";
+const char kInitialPromptFormat[] = "%s: %s\n";
 
 }  // namespace
 
-AITextSession::Context::ContextItem::ContextItem() = default;
-AITextSession::Context::ContextItem::ContextItem(const ContextItem&) = default;
-AITextSession::Context::ContextItem::ContextItem(ContextItem&&) = default;
-AITextSession::Context::ContextItem::~ContextItem() = default;
-
 using ModelExecutionError = optimization_guide::
     OptimizationGuideModelExecutionError::ModelExecutionError;
 
 AITextSession::Context::Context(uint32_t max_tokens,
-                                ContextItem initial_prompts,
-                                bool use_prompt_api_proto)
-    : max_tokens_(max_tokens),
-      initial_prompts_(std::move(initial_prompts)),
-      use_prompt_api_proto_(use_prompt_api_proto) {
-  CHECK_GE(max_tokens_, initial_prompts_.tokens)
-      << "the caller shouldn't create an AITextSession with the initial "
-         "prompts containing more tokens than the limit.";
-  current_tokens_ += initial_prompts.tokens;
+                                std::optional<ContextItem> initial_prompts)
+    : max_tokens_(max_tokens), initial_prompts_(initial_prompts) {
+  if (initial_prompts.has_value()) {
+    CHECK_GE(max_tokens_, initial_prompts->tokens)
+        << "the caller shouldn't create an AITextSession with the initial "
+           "prompts containing more tokens than the limit.";
+    current_tokens_ += initial_prompts->tokens;
+  }
 }
 
 AITextSession::Context::Context(const Context& context) = default;
@@ -130,26 +84,20 @@ void AITextSession::Context::AddContextItem(ContextItem context_item) {
   }
 }
 
-std::unique_ptr<google::protobuf::MessageLite>
-AITextSession::Context::MaybeFormatRequest(PromptApiRequest request) {
-  if (use_prompt_api_proto_) {
-    return std::make_unique<PromptApiRequest>(std::move(request));
+std::string AITextSession::Context::GetContextString() {
+  std::string context;
+  if (initial_prompts_.has_value()) {
+    context =
+        base::StringPrintf(kSystemPromptFormat, initial_prompts_->text.c_str());
   }
-  return ToStringValue(request);
-}
-
-std::unique_ptr<google::protobuf::MessageLite>
-AITextSession::Context::MakeRequest() {
-  PromptApiRequest request;
-  request.mutable_initial_prompts()->MergeFrom(initial_prompts_.prompts);
   for (auto& context_item : context_items_) {
-    request.mutable_prompt_history()->MergeFrom((context_item.prompts));
+    context.append(context_item.text);
   }
-  return MaybeFormatRequest(std::move(request));
+  return context;
 }
 
 bool AITextSession::Context::HasContextItem() {
-  return current_tokens_;
+  return initial_prompts_.has_value() || !context_items_.empty();
 }
 
 AITextSession::AITextSession(
@@ -172,8 +120,7 @@ AITextSession::AITextSession(
   // If the context is not provided, initialize a new context with the default
   // configuration.
   context_ = std::make_unique<Context>(
-      session_->GetTokenLimits().max_context_tokens, Context::ContextItem(),
-      ParseMetadata(session_->GetOnDeviceFeatureMetadata()).version() >= 1);
+      session_->GetTokenLimits().max_context_tokens, std::nullopt);
 }
 
 AITextSession::~AITextSession() = default;
@@ -182,19 +129,18 @@ void AITextSession::SetInitialPrompts(
     const std::optional<std::string> system_prompt,
     std::vector<blink::mojom::AIAssistantInitialPromptPtr> initial_prompts,
     CreateTextSessionCallback callback) {
-  PromptApiRequest request;
-  if (system_prompt) {
-    *request.add_initial_prompts() =
-        MakePrompt(PromptApiRole::PROMPT_API_ROLE_SYSTEM, *system_prompt);
+  std::string initial_prompts_str = base::StringPrintf(
+      kSystemPromptFormat, system_prompt.value_or("").c_str());
+  for (auto& initial_prompt : initial_prompts) {
+    initial_prompts_str += base::StringPrintf(
+        kInitialPromptFormat, FormatPromptRole(initial_prompt->role),
+        initial_prompt->content.c_str());
   }
-  for (const auto& prompt : initial_prompts) {
-    *request.add_initial_prompts() =
-        MakePrompt(ConvertRole(prompt->role), prompt->content);
-  }
-  session_->GetContextSizeInTokens(
-      *context_->MaybeFormatRequest(request),
+  base::TrimString(initial_prompts_str, "\n", &initial_prompts_str);
+  session_->GetSizeInTokens(
+      initial_prompts_str,
       base::BindOnce(&AITextSession::InitializeContextWithInitialPrompts,
-                     weak_ptr_factory_.GetWeakPtr(), request,
+                     weak_ptr_factory_.GetWeakPtr(), initial_prompts_str,
                      std::move(callback)));
 }
 
@@ -203,7 +149,7 @@ void AITextSession::SetDeletionCallback(base::OnceClosure deletion_callback) {
 }
 
 void AITextSession::InitializeContextWithInitialPrompts(
-    optimization_guide::proto::PromptApiRequest initial_request,
+    const std::string& initial_prompts_text,
     CreateTextSessionCallback callback,
     uint32_t size) {
   // If the on device model service fails to get the size, it will be 0.
@@ -214,7 +160,7 @@ void AITextSession::InitializeContextWithInitialPrompts(
     return;
   }
 
-  uint32_t max_token = context_->max_tokens();
+  uint32_t max_token = session_->GetTokenLimits().max_context_tokens;
   if (size > max_token) {
     // The session cannot be created if the system prompt contains more tokens
     // than the limit.
@@ -222,33 +168,27 @@ void AITextSession::InitializeContextWithInitialPrompts(
     return;
   }
 
-  auto initial_prompts = Context::ContextItem();
-  initial_prompts.tokens = size;
-  initial_prompts.prompts.Swap(initial_request.mutable_initial_prompts());
-  context_ = std::make_unique<Context>(max_token, std::move(initial_prompts),
-                                       context_->use_prompt_api_proto());
+  context_ = std::make_unique<Context>(
+      max_token, Context::ContextItem{initial_prompts_text, size});
   std::move(callback).Run(GetTextSessionInfo());
 }
 
-void AITextSession::AddPromptHistoryAndSendCompletion(
-    const PromptApiRequest& history_request,
+void AITextSession::OnGetSizeInTokensComplete(
+    const std::string& text,
     blink::mojom::ModelStreamingResponder* responder,
     uint32_t size) {
   // If the on device model service fails to get the size, it will be 0.
   // TODO(crbug.com/351935691): make sure the error is explicitly returned and
   // handled accordingly.
   if (size) {
-    auto item = Context::ContextItem();
-    item.tokens = size;
-    item.prompts = history_request.prompt_history();
-    context_->AddContextItem(std::move(item));
+    context_->AddContextItem({text, size});
   }
   responder->OnResponse(blink::mojom::ModelStreamingResponseStatus::kComplete,
                         std::nullopt, context_->current_tokens());
 }
 
 void AITextSession::ModelExecutionCallback(
-    const PromptApiRequest& input,
+    const std::string& input,
     mojo::RemoteSetElementId responder_id,
     optimization_guide::OptimizationGuideModelStreamingExecutionResult result) {
   blink::mojom::ModelStreamingResponder* responder =
@@ -271,17 +211,15 @@ void AITextSession::ModelExecutionCallback(
                           response->value(), /*current_tokens=*/std::nullopt);
   }
   if (result.response->is_complete) {
+    std::string new_context = base::StringPrintf(kContextFormat, input.c_str(),
+                                                 response->value().c_str());
     // TODO(crbug.com/351935390): instead of calculating this from the
-    // AITextSession, it should be returned by the model since the token
-    // should be calculated during the execution.
-    PromptApiRequest request;
-    request.mutable_prompt_history()->CopyFrom(input.current_prompts());
-    *request.add_prompt_history() =
-        MakePrompt(PromptApiRole::PROMPT_API_ROLE_ASSISTANT, response->value());
-    session_->GetContextSizeInTokens(
-        *context_->MaybeFormatRequest(request),
-        base::BindOnce(&AITextSession::AddPromptHistoryAndSendCompletion,
-                       weak_ptr_factory_.GetWeakPtr(), request, responder));
+    // AITextSession, it should be returned by the model since the token should
+    // be calculated during the execution.
+    session_->GetSizeInTokens(
+        new_context,
+        base::BindOnce(&AITextSession::OnGetSizeInTokensComplete,
+                       weak_ptr_factory_.GetWeakPtr(), new_context, responder));
   }
 }
 
@@ -299,27 +237,32 @@ void AITextSession::Prompt(
   }
 
   if (context_->HasContextItem()) {
-    session_->AddContext(*context_->MakeRequest());
+    optimization_guide::proto::StringValue context;
+    context.set_value(context_->GetContextString());
+    session_->AddContext(context);
   }
 
   mojo::RemoteSetElementId responder_id =
       responder_set_.Add(std::move(pending_responder));
-  PromptApiRequest request;
-  *request.add_current_prompts() =
-      MakePrompt(PromptApiRole::PROMPT_API_ROLE_USER, input);
+  optimization_guide::proto::StringValue request;
+  const std::string formatted_input = base::StringPrintf(
+      kPromptFormat,
+      FormatPromptRole(blink::mojom::AIAssistantInitialPromptRole::kUser),
+      input.c_str(),
+      FormatPromptRole(blink::mojom::AIAssistantInitialPromptRole::kAssistant));
+  request.set_value(formatted_input);
   session_->ExecuteModel(
-      *context_->MaybeFormatRequest(request),
-      base::BindRepeating(&AITextSession::ModelExecutionCallback,
-                          weak_ptr_factory_.GetWeakPtr(), request,
-                          responder_id));
+      request, base::BindRepeating(&AITextSession::ModelExecutionCallback,
+                                   weak_ptr_factory_.GetWeakPtr(),
+                                   formatted_input, responder_id));
 }
 
 void AITextSession::Fork(
     mojo::PendingReceiver<blink::mojom::AITextSession> session,
     ForkCallback callback) {
   if (!browser_context_) {
-    // The `browser_context_` is already destroyed before the renderer owner
-    // is gone.
+    // The `browser_context_` is already destroyed before the renderer owner is
+    // gone.
     std::move(callback).Run(nullptr);
     return;
   }
@@ -360,3 +303,17 @@ blink::mojom::AITextSessionInfoPtr AITextSession::GetTextSessionInfo() {
       blink::mojom::AITextSessionSamplingParams::New(
           session_sampling_params.top_k, session_sampling_params.temperature));
 }
+
+// static
+const char* AITextSession::FormatPromptRole(
+    blink::mojom::AIAssistantInitialPromptRole role) {
+  switch (role) {
+    case blink::mojom::AIAssistantInitialPromptRole::kSystem:
+      return "System";
+    case blink::mojom::AIAssistantInitialPromptRole::kUser:
+      return "User";
+    case blink::mojom::AIAssistantInitialPromptRole::kAssistant:
+      return "Model";
+  }
+  NOTREACHED();
+}
diff --git a/chrome/browser/ai/ai_text_session.h b/chrome/browser/ai/ai_text_session.h
index 6da28135a6d63..e860415d876ae 100644
--- a/chrome/browser/ai/ai_text_session.h
+++ b/chrome/browser/ai/ai_text_session.h
@@ -14,7 +14,6 @@
 #include "chrome/browser/ai/ai_context_bound_object.h"
 #include "chrome/browser/ai/ai_context_bound_object_set.h"
 #include "components/optimization_guide/core/optimization_guide_model_executor.h"
-#include "components/optimization_guide/proto/features/prompt_api.pb.h"
 #include "content/public/browser/browser_context.h"
 #include "content/public/browser/render_frame_host.h"
 #include "mojo/public/cpp/bindings/pending_receiver.h"
@@ -30,8 +29,6 @@
 class AITextSession : public AIContextBoundObject,
                       public blink::mojom::AITextSession {
  public:
-  using PromptApiPrompt = optimization_guide::proto::PromptApiPrompt;
-  using PromptApiRequest = optimization_guide::proto::PromptApiRequest;
   using CreateTextSessionCallback =
       base::OnceCallback<void(blink::mojom::AITextSessionInfoPtr)>;
 
@@ -43,18 +40,11 @@ class AITextSession : public AIContextBoundObject,
     // The structure storing the text in context and the number of tokens in the
     // text.
     struct ContextItem {
-      ContextItem();
-      ContextItem(const ContextItem&);
-      ContextItem(ContextItem&&);
-      ~ContextItem();
-
-      google::protobuf::RepeatedPtrField<PromptApiPrompt> prompts;
-      uint32_t tokens = 0;
+      const std::string text;
+      uint32_t tokens;
     };
 
-    Context(uint32_t max_tokens,
-            ContextItem initial_prompts,
-            bool use_prompt_api_proto);
+    Context(uint32_t max_tokens, std::optional<ContextItem> system_prompt);
     Context(const Context&);
     ~Context();
 
@@ -62,32 +52,22 @@ class AITextSession : public AIContextBoundObject,
     // total number of tokens in the context is below the limit.
     void AddContextItem(ContextItem context_item);
 
-    // Combines the initial prompts and all current items into a request.
-    // The type of request produced is either PromptApiRequest or StringValue,
-    // depending on use_prompt_api_proto = true.
-    std::unique_ptr<google::protobuf::MessageLite> MakeRequest();
-
-    // Either returns it's argument wrapped in unique_ptr, or converts it to a
-    // StringValue depending on whether this Context has
-    // use_prompt_api_proto = true.
-    std::unique_ptr<google::protobuf::MessageLite> MaybeFormatRequest(
-        PromptApiRequest request);
-
+    // Puts all the texts in the context together into a string.
+    std::string GetContextString();
     // Returns true if the system prompt is set or there is at least one context
     // item.
     bool HasContextItem();
+    // Clone a context with the same content.
+    std::unique_ptr<Context> CloneContext();
 
     uint32_t max_tokens() const { return max_tokens_; }
     uint32_t current_tokens() const { return current_tokens_; }
-    bool use_prompt_api_proto() const { return use_prompt_api_proto_; }
 
    private:
     uint32_t max_tokens_;
     uint32_t current_tokens_ = 0;
-    ContextItem initial_prompts_;
+    std::optional<ContextItem> initial_prompts_;
     std::deque<ContextItem> context_items_;
-    // Whether this should use PromptApiRequest or StringValue as request type.
-    bool use_prompt_api_proto_;
   };
 
   // The `AITextSession` will be owned by the `AITextSessionSet` which is bound
@@ -131,30 +111,35 @@ class AITextSession : public AIContextBoundObject,
       CreateTextSessionCallback callback);
   blink::mojom::AITextSessionInfoPtr GetTextSessionInfo();
 
+  static const char* FormatPromptRole(
+      blink::mojom::AIAssistantInitialPromptRole role);
+
  private:
   void ModelExecutionCallback(
-      const PromptApiRequest& input,
+      const std::string& input,
       mojo::RemoteSetElementId responder_id,
       optimization_guide::OptimizationGuideModelStreamingExecutionResult
           result);
 
   void InitializeContextWithInitialPrompts(
-      optimization_guide::proto::PromptApiRequest request,
+      const std::string& initial_prompts_text,
       CreateTextSessionCallback callback,
       uint32_t size);
 
   // This function is passed as a completion callback to the
   // `GetSizeInTokens()`. It will
-  // - Add the item into context, and remove the oldest items to reduce the
+  // - Add the text into context, and remove the oldest tokens to reduce the
   // context size if the number of tokens in the current context exceeds the
   // limit.
   // - Signal the completion of model execution through the `responder` with the
-  // new size of the context.
-  void AddPromptHistoryAndSendCompletion(
-      const PromptApiRequest& history_item,
+  // size returned from the `GetSizeInTokens()`.
+  void OnGetSizeInTokensComplete(
+      const std::string& text,
       blink::mojom::ModelStreamingResponder* responder,
       uint32_t size);
 
+  void GetSizeInTokens(const std::string& text,
+                       base::OnceCallback<uint32_t> callback);
   // The underlying session provided by optimization guide component.
   std::unique_ptr<optimization_guide::OptimizationGuideModelExecutor::Session>
       session_;
diff --git a/chrome/browser/ai/ai_text_session_unittest.cc b/chrome/browser/ai/ai_text_session_unittest.cc
index 2d921fda40471..83311298cb0d8 100644
--- a/chrome/browser/ai/ai_text_session_unittest.cc
+++ b/chrome/browser/ai/ai_text_session_unittest.cc
@@ -7,14 +7,11 @@
 #include <optional>
 
 #include "base/functional/callback_helpers.h"
-#include "base/notreached.h"
 #include "base/strings/stringprintf.h"
 #include "base/test/bind.h"
 #include "chrome/browser/ai/ai_test_utils.h"
 #include "components/optimization_guide/core/mock_optimization_guide_model_executor.h"
 #include "components/optimization_guide/core/optimization_guide_model_executor.h"
-#include "components/optimization_guide/proto/common_types.pb.h"
-#include "components/optimization_guide/proto/features/prompt_api.pb.h"
 #include "components/optimization_guide/proto/string_value.pb.h"
 #include "testing/gmock/include/gmock/gmock.h"
 #include "testing/gtest/include/gtest/gtest.h"
@@ -29,9 +26,6 @@ using Role = blink::mojom::AIAssistantInitialPromptRole;
 
 namespace {
 
-using optimization_guide::proto::PromptApiRequest;
-using optimization_guide::proto::PromptApiRole;
-
 const uint32_t kTestMaxContextToken = 10u;
 const uint32_t kTestInitialPromptsToken = 5u;
 const uint32_t kDefaultTopK = 1;
@@ -66,98 +60,21 @@ std::vector<blink::mojom::AIAssistantInitialPromptPtr> GetTestInitialPrompts() {
   return initial_prompts;
 }
 
-std::string GetContextString(AITextSession::Context& ctx) {
-  auto msg = ctx.MakeRequest();
-  auto* v = static_cast<optimization_guide::proto::StringValue*>(msg.get());
-  return v->value();
-}
-
-AITextSession::Context::ContextItem SimpleContextItem(std::string text,
-                                                      uint32_t size) {
-  auto item = AITextSession::Context::ContextItem();
-  item.tokens = size;
-  auto* prompt = item.prompts.Add();
-  prompt->set_role(PromptApiRole::PROMPT_API_ROLE_SYSTEM);
-  prompt->set_content(text);
-  return item;
-}
-
-const char* FormatPromptRole(PromptApiRole role) {
-  switch (role) {
-    case PromptApiRole::PROMPT_API_ROLE_SYSTEM:
-      return "S: ";
-    case PromptApiRole::PROMPT_API_ROLE_USER:
-      return "U: ";
-    case PromptApiRole::PROMPT_API_ROLE_ASSISTANT:
-      return "M: ";
-    default:
-      NOTREACHED();
-  }
-}
-
-std::string ToString(const PromptApiRequest& request) {
-  std::ostringstream oss;
-  for (const auto& prompt : request.initial_prompts()) {
-    oss << FormatPromptRole(prompt.role()) << prompt.content() << "\n";
-  }
-  for (const auto& prompt : request.prompt_history()) {
-    oss << FormatPromptRole(prompt.role()) << prompt.content() << "\n";
-  }
-  for (const auto& prompt : request.current_prompts()) {
-    oss << FormatPromptRole(prompt.role()) << prompt.content() << "\n";
-  }
-  if (request.current_prompts_size() > 0) {
-    oss << FormatPromptRole(PromptApiRole::PROMPT_API_ROLE_ASSISTANT);
-  }
-  return oss.str();
-}
-
-std::string ToString(const google::protobuf::MessageLite& request_metadata) {
-  if (request_metadata.GetTypeName() ==
-      "optimization_guide.proto.PromptApiRequest") {
-    return ToString(*static_cast<const PromptApiRequest*>(&request_metadata));
-  }
-  if (request_metadata.GetTypeName() ==
-      "optimization_guide.proto.StringValue") {
-    return static_cast<const optimization_guide::proto::StringValue*>(
-               &request_metadata)
-        ->value();
-  }
-  return "unexpected type";
-}
-
-const optimization_guide::proto::Any& GetPromptApiMetadata() {
-  static base::NoDestructor<optimization_guide::proto::Any> data([]() {
-    optimization_guide::proto::PromptApiMetadata metadata;
-    metadata.set_version(1);
-    optimization_guide::proto::Any any;
-    any.set_type_url("type.googleapis.com/" + metadata.GetTypeName());
-    any.set_value(metadata.SerializeAsString());
-    return any;
-  }());
-  return *data;
-}
-
 }  // namespace
 
 class AITextSessionTest : public AITestUtils::AITestBase {
- public:
-  struct Options {
-    blink::mojom::AITextSessionSamplingParamsPtr sampling_params = nullptr;
-    std::optional<std::string> system_prompt = std::nullopt;
-    std::vector<blink::mojom::AIAssistantInitialPromptPtr> initial_prompts;
-    std::string prompt_input = kTestPrompt;
-    std::string expected_context = "";
-    std::string expected_prompt = kExpectedFormattedTestPrompt;
-    bool use_prompt_api_proto = false;
-  };
-
  protected:
   // The helper function that creates a `AITextSession` and executes the prompt.
-  void RunPromptTest(Options options) {
+  void RunPromptTest(
+      const std::string& prompt_input,
+      blink::mojom::AITextSessionSamplingParamsPtr sampling_params,
+      const std::optional<std::string>& system_prompt,
+      std::vector<blink::mojom::AIAssistantInitialPromptPtr> initial_prompts,
+      const std::string& expected_context,
+      const std::string& expected_prompt) {
     blink::mojom::AITextSessionSamplingParamsPtr sampling_params_copy;
-    if (options.sampling_params) {
-      sampling_params_copy = options.sampling_params->Clone();
+    if (sampling_params) {
+      sampling_params_copy = sampling_params->Clone();
     }
 
     // Set up mock service.
@@ -178,10 +95,6 @@ class AITextSessionTest : public AITestUtils::AITestBase {
 
           ON_CALL(*session, GetTokenLimits())
               .WillByDefault(AITestUtils::GetFakeTokenLimits);
-          ON_CALL(*session, GetOnDeviceFeatureMetadata())
-              .WillByDefault(options.use_prompt_api_proto
-                                 ? GetPromptApiMetadata
-                                 : AITestUtils::GetFakeFeatureMetadata);
           ON_CALL(*session, GetSamplingParams()).WillByDefault([]() {
             // We don't need to use these value, so just mock it with defaults.
             return optimization_guide::SamplingParams{
@@ -195,27 +108,28 @@ class AITextSessionTest : public AITestUtils::AITestBase {
                          OptimizationGuideModelSizeInTokenCallback callback) {
                     std::move(callback).Run(text.size());
                   });
-          ON_CALL(*session, GetContextSizeInTokens(_, _))
-              .WillByDefault(
-                  [](const google::protobuf::MessageLite& request_metadata,
-                     optimization_guide::
-                         OptimizationGuideModelSizeInTokenCallback callback) {
-                    std::move(callback).Run(ToString(request_metadata).size());
-                  });
+
           ON_CALL(*session, AddContext(_))
-              .WillByDefault(
-                  [&](const google::protobuf::MessageLite& request_metadata) {
-                    EXPECT_THAT(ToString(request_metadata),
-                                options.expected_context);
-                  });
+              .WillByDefault([&](const google::protobuf::MessageLite&
+                                     request_metadata) {
+                EXPECT_THAT(
+                    static_cast<const optimization_guide::proto::StringValue*>(
+                        &request_metadata)
+                        ->value(),
+                    expected_context.c_str());
+              });
           EXPECT_CALL(*session, ExecuteModel(_, _))
               .WillOnce(
                   [&](const google::protobuf::MessageLite& request_metadata,
                       optimization_guide::
                           OptimizationGuideModelExecutionResultStreamingCallback
                               callback) {
-                    EXPECT_THAT(ToString(request_metadata),
-                                options.expected_prompt);
+                    EXPECT_THAT(
+                        static_cast<
+                            const optimization_guide::proto::StringValue*>(
+                            &request_metadata)
+                            ->value(),
+                        expected_prompt);
                     callback.Run(CreateExecutionResult(kTestResponse,
                                                        /*is_complete=*/true));
                   });
@@ -225,9 +139,8 @@ class AITextSessionTest : public AITestUtils::AITestBase {
     mojo::Remote<blink::mojom::AIManager> ai_manager = GetAIManagerRemote();
     mojo::Remote<blink::mojom::AITextSession> mock_session;
     ai_manager->CreateTextSession(
-        mock_session.BindNewPipeAndPassReceiver(),
-        std::move(options.sampling_params), options.system_prompt,
-        std::move(options.initial_prompts), base::NullCallback());
+        mock_session.BindNewPipeAndPassReceiver(), std::move(sampling_params),
+        system_prompt, std::move(initial_prompts), base::NullCallback());
 
     AITestUtils::MockModelStreamingResponder mock_responder;
 
@@ -250,7 +163,7 @@ class AITextSessionTest : public AITestUtils::AITestBase {
           run_loop.Quit();
         });
 
-    mock_session->Prompt(options.prompt_input,
+    mock_session->Prompt(prompt_input,
                          mock_responder.BindNewPipeAndPassRemote());
     run_loop.Run();
   }
@@ -277,67 +190,41 @@ class AITextSessionTest : public AITestUtils::AITestBase {
 };
 
 TEST_F(AITextSessionTest, PromptDefaultSession) {
-  RunPromptTest(AITextSessionTest::Options{
-      .prompt_input = kTestPrompt,
-      .expected_prompt = kExpectedFormattedTestPrompt,
-  });
+  RunPromptTest(kTestPrompt, /*sampling_params=*/nullptr,
+                /*system_prompt=*/std::nullopt, /*initial_prompts=*/{},
+                /*expected_context=*/"", kExpectedFormattedTestPrompt);
 }
 
 TEST_F(AITextSessionTest, PromptSessionWithSamplingParams) {
-  RunPromptTest(AITextSessionTest::Options{
-      .sampling_params = blink::mojom::AITextSessionSamplingParams::New(
-          /*top_k=*/10, /*temperature=*/0.6),
-      .prompt_input = kTestPrompt,
-      .expected_prompt = kExpectedFormattedTestPrompt,
-  });
+  RunPromptTest(kTestPrompt,
+                blink::mojom::AITextSessionSamplingParams::New(
+                    /*top_k=*/10, /*temperature=*/0.6),
+                /*system_prompt=*/std::nullopt, /*initial_prompts=*/{},
+                /*expected_context=*/"", kExpectedFormattedTestPrompt);
 }
 
 TEST_F(AITextSessionTest, PromptSessionWithSystemPrompt) {
-  RunPromptTest(AITextSessionTest::Options{
-      .system_prompt = kTestSystemPrompts,
-      .prompt_input = kTestPrompt,
-      .expected_context = kExpectedFormattedSystemPrompts,
-      .expected_prompt = kExpectedFormattedTestPrompt,
-  });
+  RunPromptTest(kTestPrompt, /*sampling_params=*/nullptr, kTestSystemPrompts,
+                /*initial_prompts=*/{}, kExpectedFormattedSystemPrompts,
+                kExpectedFormattedTestPrompt);
 }
 
 TEST_F(AITextSessionTest, PromptSessionWithInitialPrompts) {
-  RunPromptTest(AITextSessionTest::Options{
-      .initial_prompts = GetTestInitialPrompts(),
-      .prompt_input = kTestPrompt,
-      .expected_context = kExpectedFormattedInitialPrompts,
-      .expected_prompt = kExpectedFormattedTestPrompt,
-  });
+  RunPromptTest(kTestPrompt, /*sampling_params=*/nullptr,
+                /*system_prompt=*/std::nullopt, GetTestInitialPrompts(),
+                kExpectedFormattedInitialPrompts, kExpectedFormattedTestPrompt);
 }
 
 TEST_F(AITextSessionTest, PromptSessionWithSystemPromptAndInitialPrompts) {
-  RunPromptTest(AITextSessionTest::Options{
-      .system_prompt = kTestSystemPrompts,
-      .initial_prompts = GetTestInitialPrompts(),
-      .prompt_input = kTestPrompt,
-      .expected_context = kExpectedFormattedSystemPromptAndInitialPrompts,
-      .expected_prompt = kExpectedFormattedTestPrompt,
-  });
-}
-
-TEST_F(AITextSessionTest, PromptSessionWithPromptApiRequests) {
-  RunPromptTest(AITextSessionTest::Options{
-      .system_prompt = "Test system prompt",
-      .initial_prompts = GetTestInitialPrompts(),
-      .prompt_input = "Test prompt",
-      .expected_context = ("S: Test system prompt\n"
-                           "U: How are you?\n"
-                           "M: I'm fine, thank you, and you?\n"
-                           "U: I'm fine too.\n"),
-      .expected_prompt = "U: Test prompt\nM: ",
-      .use_prompt_api_proto = true,
-  });
+  RunPromptTest(kTestPrompt, /*sampling_params=*/nullptr, kTestSystemPrompts,
+                GetTestInitialPrompts(),
+                kExpectedFormattedSystemPromptAndInitialPrompts,
+                kExpectedFormattedTestPrompt);
 }
 
 // Tests `AITextSession::Context` creation without initial prompts.
 TEST(AITextSessionContextCreationTest, CreateContext_WithoutInitialPrompts) {
-  AITextSession::Context context(kTestMaxContextToken, {},
-                                 /*use_prompt_api_request*/ false);
+  AITextSession::Context context(kTestMaxContextToken, std::nullopt);
   EXPECT_FALSE(context.HasContextItem());
 }
 
@@ -345,9 +232,8 @@ TEST(AITextSessionContextCreationTest, CreateContext_WithoutInitialPrompts) {
 TEST(AITextSessionContextCreationTest,
      CreateContext_WithInitialPrompts_Normal) {
   AITextSession::Context context(
-      kTestMaxContextToken,
-      SimpleContextItem("initial prompts\n", kTestInitialPromptsToken),
-      /*use_prompt_api_request*/ false);
+      kTestMaxContextToken, AITextSession::Context::ContextItem{
+                                "initial prompts\n", kTestInitialPromptsToken});
   EXPECT_TRUE(context.HasContextItem());
 }
 
@@ -355,12 +241,12 @@ TEST(AITextSessionContextCreationTest,
 // max token limit.
 TEST(AITextSessionContextCreationTest,
      CreateContext_WithInitialPrompts_Overflow) {
-  EXPECT_DEATH_IF_SUPPORTED(AITextSession::Context context(
-                                kTestMaxContextToken,
-                                SimpleContextItem("long initial prompts\n",
-                                                  kTestMaxContextToken + 1u),
-                                /*use_prompt_api_request*/ false),
-                            "");
+  EXPECT_DEATH_IF_SUPPORTED(
+      AITextSession::Context context(
+          kTestMaxContextToken,
+          AITextSession::Context::ContextItem{"long initial prompts\n",
+                                              kTestMaxContextToken + 1u}),
+      "");
 }
 
 // Tests the `AITextSession::Context` that's initialized with/without any
@@ -384,9 +270,10 @@ class AITextSessionContextTest : public testing::Test,
   AITextSession::Context context_{
       kTestMaxContextToken,
       IsInitializedWithInitialPrompts()
-          ? SimpleContextItem("initial prompts", kTestInitialPromptsToken)
-          : AITextSession::Context::ContextItem(),
-      /*use_prompt_api_request*/ false};
+          ? std::optional<
+                AITextSession::Context::ContextItem>{{"initial prompts",
+                                                      kTestInitialPromptsToken}}
+          : std::nullopt};
 };
 
 INSTANTIATE_TEST_SUITE_P(All,
@@ -399,7 +286,7 @@ INSTANTIATE_TEST_SUITE_P(All,
 
 // Tests `GetContextString()` and `HasContextItem()` when the context is empty.
 TEST_P(AITextSessionContextTest, TestContextOperation_Empty) {
-  EXPECT_EQ(GetContextString(context_), GetInitialPromptsPrefix());
+  EXPECT_EQ(context_.GetContextString(), GetInitialPromptsPrefix());
 
   if (IsInitializedWithInitialPrompts()) {
     EXPECT_TRUE(context_.HasContextItem());
@@ -411,37 +298,35 @@ TEST_P(AITextSessionContextTest, TestContextOperation_Empty) {
 // Tests `GetContextString()` and `HasContextItem()` when some items are added
 // to the context.
 TEST_P(AITextSessionContextTest, TestContextOperation_NonEmpty) {
-  context_.AddContextItem(SimpleContextItem("test", 1u));
-  EXPECT_EQ(GetContextString(context_), GetInitialPromptsPrefix() + "test\n");
+  context_.AddContextItem({"test", 1u});
+  EXPECT_EQ(context_.GetContextString(), GetInitialPromptsPrefix() + "test");
   EXPECT_TRUE(context_.HasContextItem());
 
-  context_.AddContextItem(SimpleContextItem(" test again", 2u));
-  EXPECT_EQ(GetContextString(context_),
-            GetInitialPromptsPrefix() + "test\n test again\n");
+  context_.AddContextItem({" test again", 2u});
+  EXPECT_EQ(context_.GetContextString(),
+            GetInitialPromptsPrefix() + "test test again");
   EXPECT_TRUE(context_.HasContextItem());
 }
 
 // Tests `GetContextString()` and `HasContextItem()` when the items overflow.
 TEST_P(AITextSessionContextTest, TestContextOperation_Overflow) {
-  context_.AddContextItem(SimpleContextItem("test", 1u));
-  EXPECT_EQ(GetContextString(context_), GetInitialPromptsPrefix() + "test\n");
+  context_.AddContextItem({"test", 1u});
+  EXPECT_EQ(context_.GetContextString(), GetInitialPromptsPrefix() + "test");
   EXPECT_TRUE(context_.HasContextItem());
 
   // Since the total number of tokens will exceed `kTestMaxContextToken`, the
   // old item will be evicted.
-  context_.AddContextItem(
-      SimpleContextItem("test long token", GetMaxContextToken()));
-  EXPECT_EQ(GetContextString(context_),
-            GetInitialPromptsPrefix() + "test long token\n");
+  context_.AddContextItem({"test long token", GetMaxContextToken()});
+  EXPECT_EQ(context_.GetContextString(),
+            GetInitialPromptsPrefix() + "test long token");
   EXPECT_TRUE(context_.HasContextItem());
 }
 
 // Tests `GetContextString()` and `HasContextItem()` when the items overflow on
 // the first insertion.
 TEST_P(AITextSessionContextTest, TestContextOperation_OverflowOnFirstItem) {
-  context_.AddContextItem(
-      SimpleContextItem("test very long token", GetMaxContextToken() + 1u));
-  EXPECT_EQ(GetContextString(context_), GetInitialPromptsPrefix());
+  context_.AddContextItem({"test very long token", GetMaxContextToken() + 1u});
+  EXPECT_EQ(context_.GetContextString(), GetInitialPromptsPrefix());
   if (IsInitializedWithInitialPrompts()) {
     EXPECT_TRUE(context_.HasContextItem());
   } else {
diff --git a/components/optimization_guide/core/BUILD.gn b/components/optimization_guide/core/BUILD.gn
index ade611f05ba7c..5b3f3c8764554 100644
--- a/components/optimization_guide/core/BUILD.gn
+++ b/components/optimization_guide/core/BUILD.gn
@@ -641,7 +641,6 @@ action("on_device_model_execution_proto_generator") {
     "$root_gen_dir/components/optimization_guide/proto/string_value.descriptor",
     "$root_gen_dir/components/optimization_guide/proto/tab_organization.descriptor",
     "$root_gen_dir/components/optimization_guide/proto/history_answer.descriptor",
-    "$root_gen_dir/components/optimization_guide/proto/prompt_api.descriptor",
     "$root_gen_dir/components/optimization_guide/proto/summarize.descriptor",
   ]
   outputs = [ output_proto_descriptors_cc_file ]
@@ -653,14 +652,12 @@ action("on_device_model_execution_proto_generator") {
     "--include=\"components/optimization_guide/proto/string_value.pb.h\"",
     "--include=\"components/optimization_guide/proto/features/tab_organization.pb.h\"",
     "--include=\"components/optimization_guide/proto/features/history_answer.pb.h\"",
-    "--include=\"components/optimization_guide/proto/features/prompt_api.pb.h\"",
     "--include=\"components/optimization_guide/proto/features/summarize.pb.h\"",
     "--request=.optimization_guide.proto.ComposeRequest",
     "--request=.optimization_guide.proto.TabOrganizationRequest",
     "--request=.optimization_guide.proto.HistoryAnswerRequest",
     "--request=.optimization_guide.proto.StringValue",
     "--request=.optimization_guide.proto.SummarizeRequest",
-    "--request=.optimization_guide.proto.PromptApiRequest",
     "--response=.optimization_guide.proto.ComposeResponse",
     "--response=.optimization_guide.proto.TabOrganizationResponse",
     "--response=.optimization_guide.proto.HistoryAnswerResponse",
@@ -676,7 +673,6 @@ action("on_device_model_execution_proto_generator") {
   deps = [
     "//components/optimization_guide/proto:compose_proto_descriptor",
     "//components/optimization_guide/proto:history_answer_proto_descriptor",
-    "//components/optimization_guide/proto:prompt_api_proto_descriptor",
     "//components/optimization_guide/proto:string_value_proto_descriptor",
     "//components/optimization_guide/proto:summarize_descriptor",
     "//components/optimization_guide/proto:tab_organization_proto_descriptor",
diff --git a/components/optimization_guide/core/mock_optimization_guide_model_executor.cc b/components/optimization_guide/core/mock_optimization_guide_model_executor.cc
index 7615ef4ca680c..03b0b92b58722 100644
--- a/components/optimization_guide/core/mock_optimization_guide_model_executor.cc
+++ b/components/optimization_guide/core/mock_optimization_guide_model_executor.cc
@@ -44,16 +44,8 @@ void MockSessionWrapper::GetSizeInTokens(
     OptimizationGuideModelSizeInTokenCallback callback) {
   session_->GetSizeInTokens(text, std::move(callback));
 }
-void MockSessionWrapper::GetContextSizeInTokens(
-    const google::protobuf::MessageLite& request,
-    OptimizationGuideModelSizeInTokenCallback callback) {
-  session_->GetContextSizeInTokens(request, std::move(callback));
-}
 const SamplingParams MockSessionWrapper::GetSamplingParams() const {
   return session_->GetSamplingParams();
 }
-const proto::Any& MockSessionWrapper::GetOnDeviceFeatureMetadata() const {
-  return session_->GetOnDeviceFeatureMetadata();
-}
 
 }  // namespace optimization_guide
diff --git a/components/optimization_guide/core/mock_optimization_guide_model_executor.h b/components/optimization_guide/core/mock_optimization_guide_model_executor.h
index 6c016afe45073..c2b7bf6a3d4d1 100644
--- a/components/optimization_guide/core/mock_optimization_guide_model_executor.h
+++ b/components/optimization_guide/core/mock_optimization_guide_model_executor.h
@@ -62,18 +62,10 @@ class MockSession : public OptimizationGuideModelExecutor::Session {
               GetSizeInTokens,
               (const std::string& text,
                OptimizationGuideModelSizeInTokenCallback callback));
-  MOCK_METHOD(void,
-              GetContextSizeInTokens,
-              (const google::protobuf::MessageLite& request_metadata,
-               OptimizationGuideModelSizeInTokenCallback callback));
   MOCK_METHOD(const optimization_guide::SamplingParams,
               GetSamplingParams,
               (),
               (const override));
-  MOCK_METHOD(const proto::Any&,
-              GetOnDeviceFeatureMetadata,
-              (),
-              (const override));
 };
 
 // A wrapper that passes through calls to the underlying MockSession. Allows for
@@ -98,12 +90,7 @@ class MockSessionWrapper : public OptimizationGuideModelExecutor::Session {
       const std::string& text,
       optimization_guide::OptimizationGuideModelSizeInTokenCallback callback)
       override;
-  void GetContextSizeInTokens(
-      const google::protobuf::MessageLite& request_metadata,
-      optimization_guide::OptimizationGuideModelSizeInTokenCallback callback)
-      override;
   const SamplingParams GetSamplingParams() const override;
-  const proto::Any& GetOnDeviceFeatureMetadata() const override;
 
  private:
   raw_ptr<MockSession> session_;
diff --git a/components/optimization_guide/core/model_execution/on_device_model_feature_adapter.cc b/components/optimization_guide/core/model_execution/on_device_model_feature_adapter.cc
index c89bcd5f6cfd3..b6bee1887324e 100644
--- a/components/optimization_guide/core/model_execution/on_device_model_feature_adapter.cc
+++ b/components/optimization_guide/core/model_execution/on_device_model_feature_adapter.cc
@@ -133,8 +133,4 @@ std::optional<SamplingParams> OnDeviceModelFeatureAdapter::MaybeSamplingParams()
   };
 }
 
-const proto::Any& OnDeviceModelFeatureAdapter::GetFeatureMetadata() const {
-  return config_.feature_metadata();
-}
-
 }  // namespace optimization_guide
diff --git a/components/optimization_guide/core/model_execution/on_device_model_feature_adapter.h b/components/optimization_guide/core/model_execution/on_device_model_feature_adapter.h
index 8efa7b52a8423..50b6f87d87439 100644
--- a/components/optimization_guide/core/model_execution/on_device_model_feature_adapter.h
+++ b/components/optimization_guide/core/model_execution/on_device_model_feature_adapter.h
@@ -57,8 +57,6 @@ class OnDeviceModelFeatureAdapter final
 
   std::optional<SamplingParams> MaybeSamplingParams() const;
 
-  const proto::Any& GetFeatureMetadata() const;
-
  private:
   friend class base::RefCounted<OnDeviceModelFeatureAdapter>;
   ~OnDeviceModelFeatureAdapter();
diff --git a/components/optimization_guide/core/model_execution/session_impl.cc b/components/optimization_guide/core/model_execution/session_impl.cc
index 52588e1135f4c..3cc7f2061708c 100644
--- a/components/optimization_guide/core/model_execution/session_impl.cc
+++ b/components/optimization_guide/core/model_execution/session_impl.cc
@@ -998,23 +998,6 @@ void SessionImpl::GetSizeInTokens(
   GetOrCreateSession().GetSizeInTokens(std::move(input), std::move(callback));
 }
 
-void SessionImpl::GetContextSizeInTokens(
-    const google::protobuf::MessageLite& request,
-    OptimizationGuideModelSizeInTokenCallback callback) {
-  auto input = on_device_state_->opts.adapter->ConstructInputString(
-      request, /*want_input_context=*/true);
-  if (!input) {
-    std::move(callback).Run(0);
-    return;
-  }
-  GetOrCreateSession().GetSizeInTokens(std::move(input->input),
-                                       std::move(callback));
-}
-
-const proto::Any& SessionImpl::GetOnDeviceFeatureMetadata() const {
-  return on_device_state_->opts.adapter->GetFeatureMetadata();
-}
-
 const SamplingParams SessionImpl::GetSamplingParams() const {
   return sampling_params_;
 }
diff --git a/components/optimization_guide/core/model_execution/session_impl.h b/components/optimization_guide/core/model_execution/session_impl.h
index 519c874f54572..b2900bc9569dd 100644
--- a/components/optimization_guide/core/model_execution/session_impl.h
+++ b/components/optimization_guide/core/model_execution/session_impl.h
@@ -144,7 +144,6 @@ class SessionImpl : public OptimizationGuideModelExecutor::Session,
 
   // optimization_guide::OptimizationGuideModelExecutor::Session:
   const TokenLimits& GetTokenLimits() const override;
-  const proto::Any& GetOnDeviceFeatureMetadata() const override;
   void AddContext(
       const google::protobuf::MessageLite& request_metadata) override;
   void Score(const std::string& text,
@@ -155,9 +154,6 @@ class SessionImpl : public OptimizationGuideModelExecutor::Session,
   void GetSizeInTokens(
       const std::string& text,
       OptimizationGuideModelSizeInTokenCallback callback) override;
-  void GetContextSizeInTokens(
-      const google::protobuf::MessageLite& request_metadata,
-      OptimizationGuideModelSizeInTokenCallback callback) override;
   const SamplingParams GetSamplingParams() const override;
 
   // on_device_model::mojom::StreamingResponder:
diff --git a/components/optimization_guide/core/model_execution/substitution_unittest.cc b/components/optimization_guide/core/model_execution/substitution_unittest.cc
index cb215d13e7128..76a83ef3b31b3 100644
--- a/components/optimization_guide/core/model_execution/substitution_unittest.cc
+++ b/components/optimization_guide/core/model_execution/substitution_unittest.cc
@@ -14,7 +14,6 @@
 #include "components/optimization_guide/core/model_execution/test/feature_config_builder.h"
 #include "components/optimization_guide/proto/descriptors.pb.h"
 #include "components/optimization_guide/proto/features/compose.pb.h"
-#include "components/optimization_guide/proto/features/prompt_api.pb.h"
 #include "components/optimization_guide/proto/features/tab_organization.pb.h"
 #include "components/optimization_guide/proto/substitution.pb.h"
 #include "testing/gmock/include/gmock/gmock.h"
@@ -55,27 +54,6 @@ auto TabTitle() {
   return ProtoField({2});
 }
 
-// PromptApiRequest::prompts
-auto InitialPromptsField() {
-  return ProtoField({1});
-}
-// PromptApiRequest::current_prompts
-auto PromptHistoryField() {
-  return ProtoField({2});
-}
-// PromptApiRequest::current_prompts
-auto CurrentPromptField() {
-  return ProtoField({3});
-}
-// PromptApiPrompt::role
-auto RoleField() {
-  return ProtoField({1});
-}
-// PromptApiPrompt::content
-auto ContentField() {
-  return ProtoField({2});
-}
-
 auto Condition(proto::ProtoField&& p,
                proto::OperatorType op,
                proto::Value&& val) {
@@ -110,73 +88,6 @@ auto ConditionCheckExpr(const std::string& cond_name,
   return expr;
 }
 
-auto EnumCaseConditionList(proto::ProtoField&& field, auto v) {
-  return ConditionList(
-      proto::CONDITION_EVALUATION_TYPE_OR,
-      {
-          Condition(std::move(field), proto::OPERATOR_TYPE_EQUAL_TO,
-                    Int32Proto(static_cast<uint32_t>(v))),
-      });
-}
-
-proto::PromptApiPrompt RolePrompt(proto::PromptApiRole role,
-                                  std::string content) {
-  proto::PromptApiPrompt prompt;
-  prompt.set_role(role);
-  prompt.set_content(content);
-  return prompt;
-}
-
-proto::SubstitutedString ResolvePromptApiPrompt() {
-  proto::SubstitutedString prompt_expr;
-  prompt_expr.set_string_template("%s%s%s");
-  {
-    auto* role = prompt_expr.add_substitutions();
-    auto* sys = role->add_candidates();
-    *sys->mutable_conditions() =
-        EnumCaseConditionList(RoleField(), proto::PROMPT_API_ROLE_SYSTEM);
-    sys->set_control_token(proto::CONTROL_TOKEN_SYSTEM);
-    auto* user = role->add_candidates();
-    *user->mutable_conditions() =
-        EnumCaseConditionList(RoleField(), proto::PROMPT_API_ROLE_USER);
-    user->set_control_token(proto::CONTROL_TOKEN_USER);
-    auto* assistant = role->add_candidates();
-    assistant->set_control_token(proto::CONTROL_TOKEN_MODEL);
-  }
-  *prompt_expr.add_substitutions()->add_candidates()->mutable_proto_field() =
-      ContentField();
-  prompt_expr.add_substitutions()->add_candidates()->set_control_token(
-      proto::CONTROL_TOKEN_END);
-  return prompt_expr;
-}
-
-auto PromptApiConfig() {
-  google::protobuf::RepeatedPtrField<proto::SubstitutedString> subs;
-  auto* root = subs.Add();
-  root->set_string_template("%s%s%s%s");
-  {
-    auto* range =
-        root->add_substitutions()->add_candidates()->mutable_range_expr();
-    *range->mutable_proto_field() = InitialPromptsField();
-    *range->mutable_expr() = ResolvePromptApiPrompt();
-  }
-  {
-    auto* range =
-        root->add_substitutions()->add_candidates()->mutable_range_expr();
-    *range->mutable_proto_field() = PromptHistoryField();
-    *range->mutable_expr() = ResolvePromptApiPrompt();
-  }
-  {
-    auto* range =
-        root->add_substitutions()->add_candidates()->mutable_range_expr();
-    *range->mutable_proto_field() = CurrentPromptField();
-    *range->mutable_expr() = ResolvePromptApiPrompt();
-  }
-  root->add_substitutions()->add_candidates()->set_control_token(
-      proto::CONTROL_TOKEN_MODEL);
-  return subs;
-}
-
 TEST_F(SubstitutionTest, RawString) {
   google::protobuf::RepeatedPtrField<proto::SubstitutedString> subs;
   auto* substitution = subs.Add();
@@ -429,65 +340,6 @@ TEST_F(SubstitutionTest, RepeatedCondition) {
   EXPECT_FALSE(result->should_ignore_input_context);
 }
 
-TEST_F(SubstitutionTest, PromptApiNShot) {
-  // https://github.com/explainers-by-googlers/prompt-api?tab=readme-ov-file#n-shot-prompting
-  proto::PromptApiRequest request;
-  *request.add_initial_prompts() =
-      RolePrompt(proto::PROMPT_API_ROLE_SYSTEM,
-                 "Predict up to 5 emojis as a response to a "
-                 "comment. Output emojis, comma-separated.");
-  *request.add_initial_prompts() =
-      RolePrompt(proto::PROMPT_API_ROLE_USER, "This is amazing!");
-  *request.add_initial_prompts() =
-      RolePrompt(proto::PROMPT_API_ROLE_ASSISTANT, ", ");
-  *request.add_initial_prompts() =
-      RolePrompt(proto::PROMPT_API_ROLE_USER, "LGTM");
-  *request.add_initial_prompts() =
-      RolePrompt(proto::PROMPT_API_ROLE_ASSISTANT, ", ");
-  *request.add_current_prompts() =
-      RolePrompt(proto::PROMPT_API_ROLE_USER, "Back to the drawing board");
-  auto result = CreateSubstitutions(request, PromptApiConfig());
-  ASSERT_TRUE(result.has_value());
-  EXPECT_EQ(result->ToString(),
-            "<system>Predict up to 5 emojis as a response to a comment. Output "
-            "emojis, comma-separated.<end>"
-            "<user>This is amazing!<end>"
-            "<model>, <end>"
-            "<user>LGTM<end>"
-            "<model>, <end>"
-            "<user>Back to the drawing board<end>"
-            "<model>");
-}
-
-TEST_F(SubstitutionTest, PromptApiPersistence) {
-  // https://github.com/explainers-by-googlers/prompt-api#session-persistence-and-cloning
-  proto::PromptApiRequest request;
-  *request.add_initial_prompts() = RolePrompt(
-      proto::PROMPT_API_ROLE_SYSTEM,
-      "You are a friendly, helpful assistant specialized in clothing choices.");
-  *request.add_prompt_history() =
-      RolePrompt(proto::PROMPT_API_ROLE_USER,
-                 "What should I wear today? It's sunny and I'm unsure between "
-                 "a t-shirt and a polo.");
-  *request.add_prompt_history() =
-      RolePrompt(proto::PROMPT_API_ROLE_ASSISTANT, "Wear the t-shirt!");
-  *request.add_current_prompts() =
-      RolePrompt(proto::PROMPT_API_ROLE_USER,
-                 "That sounds great, but oh no, it's actually going to rain! "
-                 "New advice??");
-  auto result = CreateSubstitutions(request, PromptApiConfig());
-  ASSERT_TRUE(result.has_value());
-  EXPECT_EQ(result->ToString(),
-            "<system>You are a friendly, helpful assistant specialized in "
-            "clothing choices.<end>"
-            "<user>What should I wear today? It's sunny and I'm unsure between "
-            "a t-shirt and a polo.<end>"
-            "<model>Wear the t-shirt!<end>"
-            "<user>That sounds great, but oh no, it's actually going to rain! "
-            "New advice??<end>"
-            "<model>");
-}
-
 }  // namespace
 
 }  // namespace optimization_guide
diff --git a/components/optimization_guide/core/optimization_guide_model_executor.h b/components/optimization_guide/core/optimization_guide_model_executor.h
index 3ebed29eec4a5..28b9f9e16364f 100644
--- a/components/optimization_guide/core/optimization_guide_model_executor.h
+++ b/components/optimization_guide/core/optimization_guide_model_executor.h
@@ -213,19 +213,8 @@ class OptimizationGuideModelExecutor {
         const std::string& text,
         OptimizationGuideModelSizeInTokenCallback callback) = 0;
 
-    // Gets the size in tokens used by request_metadata as it would be formatted
-    // by a call to `AddContext()`. The result will be passed back through the
-    // callback.
-    virtual void GetContextSizeInTokens(
-        const google::protobuf::MessageLite& request_metadata,
-        OptimizationGuideModelSizeInTokenCallback callback) = 0;
-
     // Return the sampling params for the current session.
     virtual const SamplingParams GetSamplingParams() const = 0;
-
-    // Returns the feature_metadata from the
-    // OnDeviceModelExecutionFeatureConfig.
-    virtual const proto::Any& GetOnDeviceFeatureMetadata() const = 0;
   };
 
   // Whether an on-device session can be created for `feature`. An optional
diff --git a/components/optimization_guide/proto/BUILD.gn b/components/optimization_guide/proto/BUILD.gn
index b8ee1786644fc..d5cb55090bff1 100644
--- a/components/optimization_guide/proto/BUILD.gn
+++ b/components/optimization_guide/proto/BUILD.gn
@@ -27,7 +27,6 @@ proto_library("optimization_guide_proto") {
     "features/history_search_strings.proto",
     "features/model_prototyping.proto",
     "features/product_specifications.proto",
-    "features/prompt_api.proto",
     "features/summarize.proto",
     "features/tab_organization.proto",
     "features/text_safety.proto",
@@ -127,13 +126,3 @@ proto_library("summarize_descriptor") {
   generate_descriptor = "summarize.descriptor"
   proto_deps = [ ":optimization_guide_proto" ]
 }
-
-proto_library("prompt_api_proto_descriptor") {
-  proto_in_dir = "//"
-  proto_out_dir = "components/optimization_guide/proto"
-  sources = [ "features/prompt_api.proto" ]
-  generate_cc = false
-  generate_python = false
-  generate_descriptor = "prompt_api.descriptor"
-  proto_deps = [ ":optimization_guide_proto" ]
-}
diff --git a/components/optimization_guide/proto/features/prompt_api.proto b/components/optimization_guide/proto/features/prompt_api.proto
deleted file mode 100644
index 9c43d224f68a7..0000000000000
--- a/components/optimization_guide/proto/features/prompt_api.proto
+++ /dev/null
@@ -1,45 +0,0 @@
-// Copyright 2023 The Chromium Authors
-// Use of this source code is governed by a BSD-style license that can be
-// found in the LICENSE file.
-
-syntax = "proto3";
-
-package optimization_guide.proto;
-
-import "components/optimization_guide/proto/features/common_quality_data.proto";
-
-option optimize_for = LITE_RUNTIME;
-option java_package = "org.chromium.components.optimization_guide.features.proto";
-
-option java_outer_classname = "PromptApi";
-
-enum PromptApiRole {
-  PROMPT_API_ROLE_UNSPECIFIED = 0;
-  PROMPT_API_ROLE_SYSTEM = 1;
-  PROMPT_API_ROLE_ASSISTANT = 2;
-  PROMPT_API_ROLE_USER = 3;
-}
-
-message PromptApiPrompt {
-  PromptApiRole role = 1;
-  string content = 2;
-}
-
-// A request from PromptApi for prompt execution call.
-message PromptApiRequest {
-  // The values passed as initialPrompts for the session.
-  repeated PromptApiPrompt initial_prompts = 1;
-
-  // Previously executed prompts, and their responses.
-  repeated PromptApiPrompt prompt_history = 2;
-
-  // The new prompts for the current execution request.
-  repeated PromptApiPrompt current_prompts = 3;
-}
-
-// Type for OnDeviceModelExecutionFeatureConfig::feature_metadata.
-message PromptApiMetadata {
-  // A version for config changes that require implementor updates.
-  // Version 0->1 Changes from using StringValue -> PromptApiRequest.
-  uint32 version = 1;
-}
diff --git a/components/optimization_guide/proto/on_device_model_execution_config.proto b/components/optimization_guide/proto/on_device_model_execution_config.proto
index 5d3c7423c813c..609cb90cd0f71 100644
--- a/components/optimization_guide/proto/on_device_model_execution_config.proto
+++ b/components/optimization_guide/proto/on_device_model_execution_config.proto
@@ -9,7 +9,6 @@ option java_outer_classname = "ModelExecutionProto";
 
 package optimization_guide.proto;
 
-import "components/optimization_guide/proto/common_types.proto";
 import "components/optimization_guide/proto/descriptors.proto";
 import "components/optimization_guide/proto/redaction.proto";
 import "components/optimization_guide/proto/substitution.proto";
@@ -46,9 +45,6 @@ message OnDeviceModelExecutionFeatureConfig {
   // Sampling parameters to use with the model.
   // These will override global defaults, but not per session configurations.
   optional SamplingParams sampling_params = 6;
-
-  // Feature defined metadata.
-  optional Any feature_metadata = 7;
 }
 
 message OnDeviceModelExecutionInputConfig {
